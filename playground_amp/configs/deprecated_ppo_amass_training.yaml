# =============================================================================
# DEPRECATED: This config file uses the old schema and is no longer maintained.
# Please use ppo_walking.yaml with the new unified schema instead.
# See: playground_amp/configs/training_runtime_config.py for new schema
# =============================================================================
#
# WildRobot Training Configuration (PPO + AMP) - OLD SCHEMA
#
# Usage:
#   uv run python playground_amp/train.py
#
# Total steps = iterations × num_envs × rollout_steps
#
# Training Insights (from 2024-12-21 run):
#   - Reward plateaus around 600 iterations (~20% of 3000)
#   - 0→10%: +254 reward (big gains), 10→27%: +25 reward (diminishing returns)
#   - Optimal stopping: ~800-1000 iterations for similar quality in 1/3 time
#   - Discriminator accuracy settles at ~0.45-0.50 (ideal for AMP)
#   - Velocity stabilizes at 0.75-0.77 m/s, episode length ~230 steps
#
# Time Estimates (RTX 5070, 1024 envs):
#   - 3000 iterations (~393M steps): ~60 hours
#   - 1000 iterations (~131M steps): ~20 hours
#   - 600 iterations (~79M steps): ~12 hours (recommended for quick experiments)

# =============================================================================
# Version (see CHANGELOG.md for details)
# =============================================================================
version: "0.8.0"
version_name: "Feature Set Refactoring"

# =============================================================================
# Environment
# =============================================================================
env:
  # Model path (relative to project root)
  model_path: assets/scene_flat_terrain.xml

  ctrl_dt: 0.02          # Control: 50Hz
  sim_dt: 0.002          # Simulation: 500Hz (10 substeps)

  # Height bounds (termination + healthy reward)
  target_height: 0.45
  min_height: 0.2
  max_height: 0.7

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Max steps per episode before forced reset
  max_episode_steps: 500

# =============================================================================
# PPO Training
# =============================================================================
ppo:
  # Parallelization
  num_envs: 1024           # Parallel environments (GPU memory dependent)
  rollout_steps: 128       # Steps per env before PPO update (2.56s at 50Hz)

  # Training duration
  iterations: 1000         # Total policy updates (reduced from 3000 - plateau at ~600)

  # Learning rate
  learning_rate: 3e-4      # Adam optimizer LR

  # Discount and advantage
  gamma: 0.99
  gae_lambda: 0.95

  # PPO clipping
  clip_epsilon: 0.2

  # Loss coefficients
  entropy_coef: 0.01
  value_loss_coef: 0.5

  # Optimization
  max_grad_norm: 0.5
  epochs: 3                # PPO epochs per iteration (reduced from 4 for minibatch fix)
  num_minibatches: 128     # Minibatch size ≈ 1024 (was 8 → 16K, too large for PPO)

  # Logging
  log_interval: 10

  # Random seed
  seed: 42

# =============================================================================
# Networks
# =============================================================================
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]

# =============================================================================
# Reward Weights
# =============================================================================
reward_weights:
  tracking_lin_vel: 5.0  # Increased from 1.5 - stronger velocity incentive
  base_height: 0.3       # Healthy reward (mapped to 'healthy' in env)
  action_rate: -0.01     # Action smoothness penalty

# =============================================================================
# AMP (Adversarial Motion Prior)
# =============================================================================
amp:
  weight: 0.5                    # Increased from 0.3 - boost AMP signal

  # v0.7.1: GMR-based reference data (68.1 seconds from AMASS)
  # Direct retargeting from human mocap - no physics validation needed
  # The discriminator learns motion style, not physical feasibility
  # See: playground_amp/docs/reference_data_generation.md
  dataset_path: playground_amp/data/walking_motions_normalized_vel.pkl

  # Discriminator (v0.6.5: Middle-ground + diagnostic)
  # v0.6.3: disc_acc=1.00 (too strong) | v0.6.4: disc_acc=0.50 (collapsed)
  # TARGET: disc_acc=0.55-0.75 (healthy adversarial game)
  discriminator_hidden: [512, 256]
  disc_lr: 8e-5                  # Middle: was 1e-4 (too high) → 5e-5 (too low) → 8e-5
  batch_size: 256
  update_steps: 2                # Middle: was 3 (too many) → 1 (too few) → 2
  r1_gamma: 10.0                 # Middle: was 5.0 (too weak) → 20.0 (too strong) → 10.0
  disc_input_noise_std: 0.0      # Disabled to test if noise erases distribution

  # v0.6.0: Policy Replay Buffer (prevents catastrophic forgetting)
  replay_buffer_size: 100000     # 0 = disabled, >0 = capacity (industry: 100K-200K)
  replay_buffer_ratio: 0.2       # FIXED: Reduced from 0.5 (less old samples when D is strong)

  # v0.7.1: GMR Reference Data Feature Parity
  # GMR reference uses: finite-diff velocities + joint-based contact estimation
  # Policy must use matching methods for discriminator parity
  # NOTE: GMR data has 29-dim (9 joints), needs conversion to 27-dim (8 joints)
  use_estimated_contacts: true   # Use joint-based contact estimation (matches GMR method)
  use_finite_diff_vel: true      # Use finite difference velocities (matches GMR method)

  # Foot contact estimation parameters (for use_estimated_contacts=true)
  contact_threshold_angle: 0.1   # Hip pitch angle threshold for contact detection (rad)
  contact_knee_scale: 0.5        # Knee angle at which confidence starts decreasing (rad, ~28°)
  contact_min_confidence: 0.3    # Minimum confidence when hip indicates contact (0-1)

  # v0.6.4: Gait Mirroring (Gold Rule for symmetric gaits)
  enable_mirror_augmentation: true  # Mirror left/right to fix asymmetric reference data

  # v0.8.0: Feature Dropping (prevent discriminator shortcuts)
  # Drop artifact-prone features early in training to force learning motion style
  # See: playground_amp/docs/AMP_FEATURE_PARITY_DESIGN.md
  feature:
    drop_contacts: false    # Drop foot contacts (4 dims) - discrete, easy to exploit
    drop_height: false      # Drop root height (1 dim) - policy/reference mismatch early
    normalize_velocity: false  # Normalize root_linvel to unit direction

# =============================================================================
# W&B Logging
# =============================================================================
wandb:
  enabled: true
  project: wildrobot-amp
  mode: online
  tags: [ppo, amp]
  log_dir: playground_amp/wandb  # Local directory for wandb files and config backup

# =============================================================================
# Checkpoints
# =============================================================================
checkpoints:
  interval: 50               # Save checkpoint every N iterations (reduced for shorter runs)
  keep_last_n: 5             # Number of recent checkpoints to keep

# =============================================================================
# Video (after training)
# =============================================================================
video:
  enabled: true
  episode_length: 500
  width: 640
  height: 480
  fps: 25

# =============================================================================
# Quick Verify (smoke test)
# =============================================================================
quick_verify:
  ppo:
    num_envs: 4
    rollout_steps: 8
    iterations: 3
  amp:
    batch_size: 16
  wandb:
    enabled: false
