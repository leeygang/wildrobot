# =============================================================================
# WildRobot PPO + AMP Unified Config
# =============================================================================
# Stage 1: Robot-Native Walking Policy (No AMP)
# v0.10.3: Forward Walking - Learn to walk at 0.5-1.0 m/s
#
# Goal: Robot walks forward with stable gait
# Exit Criteria:
#   - Episode length > 400 steps (8s survival)
#   - Forward velocity 0.5-1.0 m/s with tracking error < 0.2 m/s
#   - Fall rate < 5%
#
# Usage:
#   uv run python playground_amp/train.py --config playground_amp/configs/ppo_walking.yaml
#
# Total steps = iterations × num_envs × rollout_steps
# Example: 500 × 1024 × 128 = 65M steps
#
# See: docs/learn_first_plan.md for roadmap
# =============================================================================
version: "0.10.3"
version_name: "Forward Walking (v0.10.3)"

seed: 42

# =============================================================================
# Environment
# =============================================================================
env:
  model_path: assets/scene_flat_terrain.xml

  sim_dt: 0.002
  ctrl_dt: 0.02

  max_episode_steps: 500

  # Health / termination
  target_height: 0.45
  min_height: 0.20
  max_height: 0.70
  max_pitch: 0.8
  max_roll: 0.8

  # Commands - v0.10.3: Forward walking velocity range
  # Robot learns to walk at commanded velocities
  min_velocity: 0.3
  max_velocity: 1.0    # Target walking speed range

  # Contacts
  contact_threshold_force: 5.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

# =============================================================================
# PPO Algorithm (task learning)
# =============================================================================
ppo:
  num_envs: 1024
  rollout_steps: 128
  iterations: 500       # More iterations for walking (vs 200 for standing)

  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95

  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5

  epochs: 4
  num_minibatches: 32
  max_grad_norm: 0.5

  log_interval: 10

# =============================================================================
# AMP (disabled for Stage 1, enabled for Stage 3)
# =============================================================================
amp:
  enabled: false

  # Reference dataset (Stage 3 only)
  dataset_path: null

  # Global AMP reward mixing
  weight: 0.0          # Stage 1 = 0.0, Stage 3 > 0

  # Discriminator training (NOT architecture)
  discriminator:
    learning_rate: 8e-5
    batch_size: 256
    updates_per_ppo_update: 2
    r1_gamma: 10.0
    input_noise_std: 0.03

  # Feature parity controls
  feature_config:
    use_finite_diff_vel: true
    use_estimated_contacts: true
    mask_waist: false
    enable_mirror_augmentation: false

  # Diagnostics (for alerts/logging only)
  targets:
    disc_acc_min: 0.55
    disc_acc_max: 0.80

# =============================================================================
# Networks (Option A: algorithm-agnostic)
# =============================================================================
networks:
  actor:
    hidden_sizes: [256, 256, 128]
    activation: elu
    log_std_init: -1.0
    min_log_std: -5.0
    max_log_std: 2.0

  critic:
    hidden_sizes: [256, 256, 128]
    activation: elu

  discriminator:
    hidden_sizes: [512, 256]
    activation: relu

# =============================================================================
# Task Reward Weights (Environment-side reward only)
# v0.10.3: Balance velocity tracking with stability
# =============================================================================
reward_weights:
  # Primary objectives - v0.10.3: Velocity tracking is now important
  tracking_lin_vel: 2.0   # Increased for walking
  lateral_velocity: -0.5
  base_height: 0.5        # Reduced (was 1.0) - stability established
  orientation: -0.5       # Reduced penalty (was -1.0) - allow some motion
  angular_velocity: -0.05 # Reduced (was -0.1) - allow turning

  # Effort and safety
  torque: -0.001
  saturation: -0.1

  # Smoothness
  action_rate: -0.01
  joint_velocity: -0.001

  # Foot stability
  slip: -0.5
  clearance: 0.1

  # Shaping
  forward_velocity_scale: 4.0

# =============================================================================
# Reward Composition (Trainer-side)
# =============================================================================
reward:
  task_weight: 1.0
  amp_weight: 0.0            # Alias of amp.weight (keep in sync)
  task_reward_clip: null
  amp_reward_clip: [0.0, 1.0]

# =============================================================================
# Checkpoints
# =============================================================================
checkpoints:
  dir: playground_amp/checkpoints
  interval: 50
  keep_last_n: 5

# =============================================================================
# Logging
# =============================================================================
wandb:
  enabled: true
  project: wildrobot-ppo
  mode: online
  tags: [ppo, stage1, walking, v0.10.3]

video:
  enabled: true
  episode_length: 500
  width: 640
  height: 480
  fps: 25

# =============================================================================
# Quick Verify (Smoke Test)
# =============================================================================
quick_verify:
  enabled: false
  ppo:
    num_envs: 4
    rollout_steps: 8
    iterations: 3
  amp:
    enabled: false
