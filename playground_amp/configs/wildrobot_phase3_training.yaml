# Phase 3: RL Training (PPO + AMP) configuration
# Purpose: canonical training-stage config for Phase III (large-scale RL + AMP)
#
# Usage:
#   uv run python playground_amp/train_amp.py --use-custom-loop
#   uv run python playground_amp/train_amp.py --use-custom-loop --config configs/wildrobot_phase3_training.yaml
#
# CLI arguments override config file values when provided.

stage: Phase3

# Environment configuration
env:
  # Simulation timing
  ctrl_dt: 0.02      # Control frequency: 50Hz
  sim_dt: 0.002      # Simulation frequency: 500Hz (10 substeps per control)

  # Robot parameters
  target_height: 0.45    # Target standing height (meters)
  min_height: 0.2        # Termination: too low
  max_height: 0.7        # Termination: too high

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering (low-pass filter for smooth control)
  use_action_filter: true
  action_filter_alpha: 0.7   # alpha * prev + (1-alpha) * current

  # Episode configuration
  max_episode_steps: 500

# Trainer configuration
trainer:
  # Number of parallel environments
  # GPU Memory Guide:
  #   RTX 5070 (12GB): 512
  #   RTX 3080 (10GB): 512-1024
  #   RTX 3090 (24GB): 2048-4096
  #   RTX 4090 (24GB): 2048-4096
  num_envs: 512

  # Rollout steps per environment before policy update
  rollout_steps: 10

  # Total training iterations
  iterations: 3000

  # Learning rate
  lr: 3e-4

  # Discount factor
  gamma: 0.99

  # GAE lambda
  gae_lambda: 0.95

  # Random seed
  seed: 42

  # PPO clipping parameter
  clip_epsilon: 0.2

  # Entropy coefficient (exploration bonus)
  entropy_coef: 0.01

  # Value loss coefficient
  value_loss_coef: 0.5

  # Gradient clipping
  max_grad_norm: 0.5

  # PPO update epochs per iteration
  epochs: 4

  # Number of minibatches per epoch
  num_minibatches: 4

  # Checkpoint directory
  checkpoint_dir: playground_amp/checkpoints

  # Save checkpoint every N iterations
  save_interval: 100

  # Log metrics every N iterations
  log_interval: 10

# Network architecture
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]
  log_std_min: -20.0
  log_std_max: 2.0

# Control settings
control:
  control_freq: 50
  action_type: pd_target

# Observation settings
observations:
  noise_std: 0.02

# Reward weights
reward_weights:
  tracking_lin_vel: 1.0
  tracking_ang_vel: 0.5
  amp_style: 1.0          # Equal weight to task reward (2024 SoTA)
  torque_penalty: -0.05
  action_rate: -0.01
  dof_pos_limits: -0.1
  base_height: 0.3

# Domain randomization (per-episode sampling ranges)
domain_randomization:
  friction: [0.4, 1.25]
  mass_scale: [0.85, 1.15]
  link_mass_scale: [0.8, 1.2]
  push_force: [0.0, 10.0]
  motor_strength: [0.9, 1.1]
  joint_damping: [0.5, 1.5]
  control_latency: [0.0, 0.02]

# AMP (Adversarial Motion Prior) settings
amp:
  # Enable/disable AMP
  enabled: true

  # AMP reward weight (equal to task reward for natural gait)
  weight: 1.0

  # Discriminator architecture
  discriminator_hidden: [1024, 512, 256]

  # Path to reference motion dataset (relative to project root)
  # Generated by: cd ~/projects/GMR && uv run python scripts/batch_convert_to_amp.py
  dataset_path: playground_amp/data/walking_motions_merged.pkl

  # Reference motion mode: 'file' (load from dataset_path) or 'synthetic' (generated)
  ref_motion_mode: file

  # Reference motion buffer size
  ref_buffer_size: 2000

  # Sequence length for AMP transitions
  ref_seq_len: 32

  # Discriminator learning rate
  disc_lr: 1e-4

  # Discriminator batch size
  batch_size: 512

  # Discriminator updates per policy iteration
  update_steps: 2

  # WGAN-GP gradient penalty weight
  gradient_penalty_weight: 5.0

  # Normalize AMP features
  normalize_features: true

# Episode termination thresholds
termination:
  min_base_height: 0.25
  max_pitch_roll_rad: 0.785398  # 45 degrees

# Quick-verify settings (for smoke tests)
quick_verify:
  trainer:
    num_envs: 4
    rollout_steps: 8
    iterations: 10
  env:
    max_episode_steps: 100
  amp:
    batch_size: 32  # Smaller batch for quick verify (must fit in 4*8=32 agent samples)

notes: |
  Phase 3 PPO+AMP Training Configuration

  GPU Memory Guide for --num-envs:
    RTX 5070 (12GB): 512
    RTX 3080 (10GB): 512-1024
    RTX 3090 (24GB): 2048-4096

  Training Command:
    uv run python playground_amp/train_amp.py --use-custom-loop

  Quick Verify:
    uv run python playground_amp/train_amp.py --use-custom-loop --verify
