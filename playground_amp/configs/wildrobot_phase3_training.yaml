# Phase 3: RL Training (PPO + AMP) configuration
# Purpose: canonical training-stage config for Phase III (large-scale RL + AMP)
#
# Usage:
#   uv run python playground_amp/train_amp.py
#   uv run python playground_amp/train_amp.py --config configs/wildrobot_phase3_training.yaml
#
# CLI arguments override config file values when provided.

stage: Phase3

# Environment configuration
env:
  # Simulation timing
  ctrl_dt: 0.02      # Control frequency: 50Hz
  sim_dt: 0.002      # Simulation frequency: 500Hz (10 substeps per control)

  # Robot parameters
  target_height: 0.45    # Target standing height (meters)
  min_height: 0.2        # Termination: too low
  max_height: 0.7        # Termination: too high

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering (low-pass filter for smooth control)
  use_action_filter: true
  action_filter_alpha: 0.7   # alpha * prev + (1-alpha) * current

  # Episode configuration
  max_episode_steps: 500

# Trainer configuration
trainer:
  # Number of parallel environments
  # GPU Memory Guide (for WildRobot 9-DOF):
  #   RTX 5070 (12GB): 1024
  #   RTX 3080 (10GB): 512-1024
  #   RTX 3090 (24GB): 2048-4096
  #   RTX 4090 (24GB): 4096-8192
  # Higher = better GPU utilization, but more memory
  num_envs: 1024

  # Rollout steps per environment before policy update
  # Higher = more data per update, better sample efficiency
  # Memory: num_envs * rollout_steps * obs_dim * sizeof(float32)
  rollout_steps: 20

  # Total training iterations
  iterations: 3000

  # Learning rate
  lr: 3e-4

  # Discount factor
  gamma: 0.99

  # GAE lambda
  gae_lambda: 0.95

  # Random seed
  seed: 42

  # PPO clipping parameter
  clip_epsilon: 0.2

  # Entropy coefficient (exploration bonus)
  entropy_coef: 0.01

  # Value loss coefficient
  value_loss_coef: 0.5

  # Gradient clipping
  max_grad_norm: 0.5

  # PPO update epochs per iteration
  epochs: 4

  # Number of minibatches per epoch
  # Higher = smaller minibatch, less memory but more updates
  # Minibatch size = (num_envs * rollout_steps) / num_minibatches
  # With 2048 envs * 20 steps / 8 = 5120 samples per minibatch
  num_minibatches: 8

  # Checkpoint directory
  checkpoint_dir: playground_amp/checkpoints

  # Save checkpoint every N iterations
  save_interval: 100

  # Log metrics every N iterations
  log_interval: 10

# Network architecture
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]
  log_std_min: -20.0
  log_std_max: 2.0

# Control settings
control:
  control_freq: 50
  action_type: pd_target

# Observation settings
observations:
  noise_std: 0.02

# Reward weights
reward_weights:
  tracking_lin_vel: 1.0
  tracking_ang_vel: 0.5
  amp_style: 1.0          # Equal weight to task reward (2024 SoTA)
  torque_penalty: -0.05
  action_rate: -0.01
  dof_pos_limits: -0.1
  base_height: 0.3

# Domain randomization (per-episode sampling ranges)
domain_randomization:
  friction: [0.4, 1.25]
  mass_scale: [0.85, 1.15]
  link_mass_scale: [0.8, 1.2]
  push_force: [0.0, 10.0]
  motor_strength: [0.9, 1.1]
  joint_damping: [0.5, 1.5]
  control_latency: [0.0, 0.02]

# AMP (Adversarial Motion Prior) settings
amp:
  # Enable/disable AMP
  enabled: true

  # AMP reward weight (equal to task reward for natural gait)
  weight: 1.0

  # Discriminator architecture
  discriminator_hidden: [1024, 512, 256]

  # Path to reference motion dataset (relative to project root)
  # Generated by: cd ~/projects/GMR && uv run python scripts/batch_convert_to_amp.py
  dataset_path: playground_amp/data/walking_motions_merged.pkl

  # Reference motion mode: 'file' (load from dataset_path) or 'synthetic' (generated)
  ref_motion_mode: file

  # Reference motion buffer size
  ref_buffer_size: 2000

  # Sequence length for AMP transitions
  ref_seq_len: 32

  # Discriminator learning rate
  disc_lr: 1e-4

  # Discriminator batch size
  # Should be <= num_envs * rollout_steps for sufficient samples
  batch_size: 2048

  # Discriminator updates per policy iteration
  update_steps: 2

  # WGAN-GP gradient penalty weight
  gradient_penalty_weight: 5.0

  # Normalize AMP features
  normalize_features: true

# Episode termination thresholds
termination:
  min_base_height: 0.25
  max_pitch_roll_rad: 0.785398  # 45 degrees

# Weights & Biases (W&B) experiment tracking
wandb:
  # Enable/disable W&B logging
  enabled: true

  # W&B project name (appears in dashboard)
  project: wildrobot-amp

  # W&B entity (team/username, null for personal account)
  entity: null

  # Run name (auto-generated if null)
  name: null

  # Tags for filtering runs
  tags:
    - ppo
    - amp

  # W&B mode: "online" (sync to cloud), "offline" (local only), "disabled"
  mode: online

  # Log frequency (log every N iterations)
  log_frequency: 10

  # Local log backup directory
  log_dir: logs

# Quick-verify settings (for smoke tests) - minimal for fastest possible test
quick_verify:
  trainer:
    num_envs: 4       # Minimum vectorized
    rollout_steps: 8  # Minimum for GAE
    iterations: 3     # Just 3 iterations to verify loop works
  env:
    max_episode_steps: 20  # Very short episodes
  amp:
    batch_size: 16  # Smallest batch (must fit in 4*8=32 agent samples)
  wandb:
    enabled: false  # Disable W&B for smoke tests

# GPU memory presets (use with --num-envs CLI override)
# Uncomment the appropriate section for your GPU:
#
# RTX 5070 (12GB) - Balanced:
#   num_envs: 2048, rollout_steps: 20, batch_size: 2048
#
# RTX 5070 (12GB) - Max utilization (if no OOM):
#   num_envs: 4096, rollout_steps: 10, batch_size: 2048
#
# RTX 3090/4090 (24GB):
#   num_envs: 4096, rollout_steps: 20, batch_size: 4096

notes: |
  Phase 3 PPO+AMP Training Configuration

  GPU Memory Guide for --num-envs:
    RTX 5070 (12GB): 512
    RTX 3080 (10GB): 512-1024
    RTX 3090 (24GB): 2048-4096

  Training Command:
    uv run python playground_amp/train_amp.py

  Quick Verify:
    uv run python playground_amp/train_amp.py --verify
