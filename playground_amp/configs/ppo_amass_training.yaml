# WildRobot Training Configuration (PPO + AMP)
#
# Usage:
#   uv run python playground_amp/train.py
#
# Total steps = iterations × num_envs × rollout_steps
#
# Training Insights (from 2024-12-21 run):
#   - Reward plateaus around 600 iterations (~20% of 3000)
#   - 0→10%: +254 reward (big gains), 10→27%: +25 reward (diminishing returns)
#   - Optimal stopping: ~800-1000 iterations for similar quality in 1/3 time
#   - Discriminator accuracy settles at ~0.45-0.50 (ideal for AMP)
#   - Velocity stabilizes at 0.75-0.77 m/s, episode length ~230 steps
#
# Time Estimates (RTX 5070, 1024 envs):
#   - 3000 iterations (~393M steps): ~60 hours
#   - 1000 iterations (~131M steps): ~20 hours
#   - 600 iterations (~79M steps): ~12 hours (recommended for quick experiments)

# =============================================================================
# Version (see CHANGELOG.md for details)
# =============================================================================
version: "0.4.0"
version_name: "Critical Bug Fixes"

# =============================================================================
# Environment
# =============================================================================
env:
  # Model path (relative to project root)
  model_path: assets/scene_flat_terrain.xml

  ctrl_dt: 0.02          # Control: 50Hz
  sim_dt: 0.002          # Simulation: 500Hz (10 substeps)

  # Height bounds (termination + healthy reward)
  target_height: 0.45
  min_height: 0.2
  max_height: 0.7

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Max steps per episode before forced reset
  max_episode_steps: 500

# =============================================================================
# Training
# =============================================================================
trainer:
  num_envs: 1024         # Parallel environments (GPU memory dependent)
  rollout_steps: 128     # Steps per env before PPO update (2.56s at 50Hz)
  iterations: 1000       # Total policy updates (reduced from 3000 - plateau at ~600)

  # PPO hyperparameters
  lr: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  epochs: 3              # PPO epochs per iteration (reduced from 4 for minibatch fix)
  num_minibatches: 128   # Minibatch size ≈ 1024 (was 8 → 16K, too large for PPO)

  # Logging
  log_interval: 10
  checkpoint_dir: playground_amp/checkpoints
  seed: 42

# =============================================================================
# Networks
# =============================================================================
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]

# =============================================================================
# Reward Weights
# =============================================================================
reward_weights:
  tracking_lin_vel: 5.0  # Increased from 1.5 - stronger velocity incentive
  base_height: 0.3       # Healthy reward (mapped to 'healthy' in env)
  action_rate: -0.01     # Action smoothness penalty

# =============================================================================
# AMP (Adversarial Motion Prior)
# =============================================================================
amp:
  weight: 0.3                    # Reduced - lower style pressure until robot is smoother
  dataset_path: playground_amp/data/walking_motions_normalized_vel.pkl

  # Discriminator (middle-ground: not too strong, not collapsed)
  discriminator_hidden: [512, 256]  # Increased capacity (was [256, 128])
  disc_lr: 1e-4                  # Increased from 5e-5 - disc was collapsing
  batch_size: 256
  update_steps: 3                # Increased from 2 - give disc more updates
  gradient_penalty_weight: 5.0   # Reduced from 15.0 - was over-regularizing
  disc_input_noise_std: 0.02     # Reduced from 0.05 - too much blur

# =============================================================================
# W&B Logging
# =============================================================================
wandb:
  enabled: true
  project: wildrobot-amp
  mode: online
  tags: [ppo, amp]
  log_dir: playground_amp/wandb  # Local directory for wandb files and config backup

# =============================================================================
# Checkpoints
# =============================================================================
checkpoints:
  interval: 50               # Save checkpoint every N iterations (reduced for shorter runs)
  keep_last_n: 5             # Number of recent checkpoints to keep

# =============================================================================
# Video (after training)
# =============================================================================
video:
  enabled: true
  episode_length: 500
  width: 640
  height: 480
  fps: 25

# =============================================================================
# Quick Verify (smoke test)
# =============================================================================
quick_verify:
  trainer:
    num_envs: 4
    rollout_steps: 8
    iterations: 3
  amp:
    batch_size: 16
  wandb:
    enabled: false
