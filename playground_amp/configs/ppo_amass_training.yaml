# WildRobot Training Configuration (PPO + AMP)
#
# Usage:
#   uv run python playground_amp/train.py
#
# Total steps = iterations × num_envs × rollout_steps
#
# Training Insights (from 2024-12-21 run):
#   - Reward plateaus around 600 iterations (~20% of 3000)
#   - 0→10%: +254 reward (big gains), 10→27%: +25 reward (diminishing returns)
#   - Optimal stopping: ~800-1000 iterations for similar quality in 1/3 time
#   - Discriminator accuracy settles at ~0.45-0.50 (ideal for AMP)
#   - Velocity stabilizes at 0.75-0.77 m/s, episode length ~230 steps
#
# Time Estimates (RTX 5070, 1024 envs):
#   - 3000 iterations (~393M steps): ~60 hours
#   - 1000 iterations (~131M steps): ~20 hours
#   - 600 iterations (~79M steps): ~12 hours (recommended for quick experiments)

# =============================================================================
# Version (see CHANGELOG.md for details)
# =============================================================================
version: "0.6.6"
version_name: "Feature Parity Fix"

# =============================================================================
# Environment
# =============================================================================
env:
  # Model path (relative to project root)
  model_path: assets/scene_flat_terrain.xml

  ctrl_dt: 0.02          # Control: 50Hz
  sim_dt: 0.002          # Simulation: 500Hz (10 substeps)

  # Height bounds (termination + healthy reward)
  target_height: 0.45
  min_height: 0.2
  max_height: 0.7

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Max steps per episode before forced reset
  max_episode_steps: 500

# =============================================================================
# Training
# =============================================================================
trainer:
  num_envs: 1024         # Parallel environments (GPU memory dependent)
  rollout_steps: 128     # Steps per env before PPO update (2.56s at 50Hz)
  iterations: 1000       # Total policy updates (reduced from 3000 - plateau at ~600)

  # PPO hyperparameters
  lr: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  epochs: 3              # PPO epochs per iteration (reduced from 4 for minibatch fix)
  num_minibatches: 128   # Minibatch size ≈ 1024 (was 8 → 16K, too large for PPO)

  # Logging
  log_interval: 10
  checkpoint_dir: playground_amp/checkpoints
  seed: 42

# =============================================================================
# Networks
# =============================================================================
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]

# =============================================================================
# Reward Weights
# =============================================================================
reward_weights:
  tracking_lin_vel: 5.0  # Increased from 1.5 - stronger velocity incentive
  base_height: 0.3       # Healthy reward (mapped to 'healthy' in env)
  action_rate: -0.01     # Action smoothness penalty

# =============================================================================
# AMP (Adversarial Motion Prior)
# =============================================================================
amp:
  weight: 0.5                    # Increased from 0.3 - boost AMP signal
  dataset_path: playground_amp/data/walking_motions_normalized_vel.pkl

  # Discriminator (v0.6.5: Middle-ground + diagnostic)
  # v0.6.3: disc_acc=1.00 (too strong) | v0.6.4: disc_acc=0.50 (collapsed)
  # TARGET: disc_acc=0.55-0.75 (healthy adversarial game)
  discriminator_hidden: [512, 256]
  disc_lr: 8e-5                  # Middle: was 1e-4 (too high) → 5e-5 (too low) → 8e-5
  batch_size: 256
  update_steps: 2                # Middle: was 3 (too many) → 1 (too few) → 2
  r1_gamma: 10.0                 # Middle: was 5.0 (too weak) → 20.0 (too strong) → 10.0
  disc_input_noise_std: 0.0      # Disabled to test if noise erases distribution

  # v0.6.0: Policy Replay Buffer (prevents catastrophic forgetting)
  replay_buffer_size: 100000     # 0 = disabled, >0 = capacity (industry: 100K-200K)
  replay_buffer_ratio: 0.2       # FIXED: Reduced from 0.5 (less old samples when D is strong)

  # v0.6.2: Golden Rule Configuration (Mathematical Parity)
  # These ensure policy and reference features are computed identically
  use_estimated_contacts: true   # Use joint-based contact estimation (matches reference data)
  use_finite_diff_vel: true      # Use finite difference velocities (matches reference data)

  # Foot contact estimation parameters (for use_estimated_contacts=true)
  contact_threshold_angle: 0.1   # Hip pitch angle threshold for contact detection (rad)
  contact_knee_scale: 0.5        # Knee angle at which confidence starts decreasing (rad, ~28°)
  contact_min_confidence: 0.3    # Minimum confidence when hip indicates contact (0-1)

  # v0.6.3: Feature Cleaning
  velocity_filter_alpha: 0.0     # Disabled to test if filter erases distribution
  ankle_offset: 0.18             # Ankle pitch calibration offset (rad) - fixes neutral pose mismatch

  # v0.6.4: Gait Mirroring (Gold Rule for symmetric gaits)
  enable_mirror_augmentation: true  # Mirror left/right to fix asymmetric reference data

# =============================================================================
# W&B Logging
# =============================================================================
wandb:
  enabled: true
  project: wildrobot-amp
  mode: online
  tags: [ppo, amp]
  log_dir: playground_amp/wandb  # Local directory for wandb files and config backup

# =============================================================================
# Checkpoints
# =============================================================================
checkpoints:
  interval: 50               # Save checkpoint every N iterations (reduced for shorter runs)
  keep_last_n: 5             # Number of recent checkpoints to keep

# =============================================================================
# Video (after training)
# =============================================================================
video:
  enabled: true
  episode_length: 500
  width: 640
  height: 480
  fps: 25

# =============================================================================
# Quick Verify (smoke test)
# =============================================================================
quick_verify:
  trainer:
    num_envs: 4
    rollout_steps: 8
    iterations: 3
  amp:
    batch_size: 16
  wandb:
    enabled: false
