# WildRobot Training Configuration (PPO + AMP)
#
# Usage:
#   uv run python playground_amp/train.py
#
# Total steps = iterations × num_envs × rollout_steps
#
# Training Insights (from 2024-12-21 run):
#   - Reward plateaus around 600 iterations (~20% of 3000)
#   - 0→10%: +254 reward (big gains), 10→27%: +25 reward (diminishing returns)
#   - Optimal stopping: ~800-1000 iterations for similar quality in 1/3 time
#   - Discriminator accuracy settles at ~0.45-0.50 (ideal for AMP)
#   - Velocity stabilizes at 0.75-0.77 m/s, episode length ~230 steps
#
# Time Estimates (RTX 5070, 1024 envs):
#   - 3000 iterations (~393M steps): ~60 hours
#   - 1000 iterations (~131M steps): ~20 hours
#   - 600 iterations (~79M steps): ~12 hours (recommended for quick experiments)

# =============================================================================
# Environment
# =============================================================================
env:
  ctrl_dt: 0.02          # Control: 50Hz
  sim_dt: 0.002          # Simulation: 500Hz (10 substeps)

  # Height bounds (termination + healthy reward)
  target_height: 0.45
  min_height: 0.2
  max_height: 0.7

  # Velocity command range (m/s)
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Max steps per episode before forced reset
  max_episode_steps: 500

# =============================================================================
# Training
# =============================================================================
trainer:
  num_envs: 1024         # Parallel environments (GPU memory dependent)
  rollout_steps: 128     # Steps per env before PPO update (2.56s at 50Hz)
  iterations: 1000       # Total policy updates (reduced from 3000 - plateau at ~600)

  # PPO hyperparameters
  lr: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  epochs: 4              # PPO epochs per iteration
  num_minibatches: 8     # Minibatch size = (num_envs × rollout_steps) / num_minibatches

  # Logging
  log_interval: 10
  checkpoint_dir: playground_amp/checkpoints
  seed: 42

# =============================================================================
# Networks
# =============================================================================
networks:
  policy_hidden_dims: [512, 256, 128]
  value_hidden_dims: [512, 256, 128]

# =============================================================================
# Reward Weights
# =============================================================================
reward_weights:
  tracking_lin_vel: 1.5  # Reduced from 5.0 - less focus on raw speed
  base_height: 0.3       # Healthy reward (mapped to 'healthy' in env)
  action_rate: -0.01     # Action smoothness penalty

# =============================================================================
# AMP (Adversarial Motion Prior)
# =============================================================================
amp:
  weight: 1.0                    # High weight - prioritize natural style over raw speed
  dataset_path: playground_amp/data/walking_motions_merged.pkl

  # Discriminator
  discriminator_hidden: [256, 128]
  disc_lr: 5e-6                  # Very slow (prevents mode collapse)
  batch_size: 256
  update_steps: 2                # Reduced from 4 - give policy more room to learn
  gradient_penalty_weight: 50.0  # WGAN-GP regularization
  disc_input_noise_std: 0.05     # Input noise (hides simulation artifacts)

# =============================================================================
# W&B Logging
# =============================================================================
wandb:
  enabled: true
  project: wildrobot-amp
  mode: online
  tags: [ppo, amp]

# =============================================================================
# Checkpoints
# =============================================================================
checkpoints:
  interval: 50               # Save checkpoint every N iterations (reduced for shorter runs)
  keep_last_n: 5             # Number of recent checkpoints to keep

# =============================================================================
# Video (after training)
# =============================================================================
video:
  enabled: true
  episode_length: 500
  width: 640
  height: 480
  fps: 25

# =============================================================================
# Quick Verify (smoke test)
# =============================================================================
quick_verify:
  trainer:
    num_envs: 4
    rollout_steps: 8
    iterations: 3
  amp:
    batch_size: 16
  wandb:
    enabled: false
