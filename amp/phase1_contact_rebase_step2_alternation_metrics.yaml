# WildRobot AMP - Phase 1 Contact Rebase: Step 2 - Alternation Metrics
# Date: 2025-12-06
# Strategy: Start from Phase 0 baseline and enable alternation tracking (metrics only)
# Goal: Verify that adding the alternation metric (observability) does not change training

# Base env (same as Phase 0 baseline)
env:
  terrain: "flat"
  ctrl_dt: 0.02
  sim_dt: 0.002

  # Velocity command - Phase 0 targets
  velocity_command_mode: "range"
  target_velocity: 0.7
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Phase signal (kept on; contact shaping uses later phases)
  use_phase_signal: true
  phase_period: 40
  num_phase_clocks: 2

  # Termination conditions
  min_height: 0.2
  max_height: 0.7

# Explicit set of tracked components for clearer diagnostics
tracked_reward_components:
  - tracking_exp_xy
  - tracking_lin_xy
  - tracking_exp_yaw
  - tracking_lin_yaw
  - forward_velocity_bonus
  - velocity_threshold_penalty
  - z_velocity
  - roll_pitch_velocity
  - roll_pitch_position
  - joint_velocity
  - joint_acceleration
  - action_rate
  - nominal_joint_position
  - joint_position_limit
  - joint_torque
  - mechanical_power
  - foot_contact
  - foot_sliding
  - existential
  # Phase 1 metric: alternation ratio (metrics-only in Step 2)
  - contact_alternation_ratio

# Reward weights - Phase 0 (Foundations) - PROVEN WORKING CONFIG
reward_weights:
  # Tracking kernel
  tracking_sigma: 0.25
  tracking_steepness: 3.0

  # Velocity threshold penalty (STRICT & CONSTANT - Phase 0 success formula)
  velocity_threshold: 0.12
  velocity_threshold_penalty: 4.9
  velocity_threshold_penalty_decay_start: 0
  velocity_threshold_penalty_decay_steps: 0
  velocity_threshold_penalty_min_scale: 0.5

  # Velocity tracking (moderate)
  tracking_exp_xy: 25.0
  tracking_lin_xy: 12.0
  tracking_exp_yaw: 5.0
  tracking_lin_yaw: 1.5

  # Direct forward velocity bonus (MODERATE - sufficient with strict penalty)
  forward_velocity_bonus: 51.0

  # Stability penalties (baseline)
  z_velocity: 0.1
  roll_pitch_velocity: 0.008
  roll_pitch_position: 0.04

  # Smoothness penalties (minimal early)
  nominal_joint_position: 0.0
  joint_position_limit: 2.5
  joint_velocity: 0.0
  joint_acceleration: 1e-8
  action_rate: 0.001

  # Energy penalties (reduced)
  joint_torque: 7e-8
  mechanical_power: 7e-6

  # Contact shaping (GENTLE - Phase 0 baseline)
  foot_contact: 5.0
  foot_sliding: 0.5

  # Existential baseline & gating controls
  existential: 0.5
  tracking_gate_velocity: 0.05
  tracking_gate_scale: 0.45

training:
  num_timesteps: 2000000    # 2M steps quick verify (~45 min)
  num_evals: 40
  episode_length: 600
  seed: 0

quick_verify:
  enabled: false
  num_timesteps: 10000
  num_evals: 2
  episode_length: 100
  num_envs: 128
  num_eval_envs: 16
  use_wandb: false
  render_videos: false
  save_checkpoints: false

ppo:
  num_envs: 512
  num_eval_envs: 64
  batch_size: 256
  unroll_length: 32
  num_minibatches: 16
  num_updates_per_batch: 4

  learning_rate: 0.0003
  entropy_cost: 0.01
  discounting: 0.99
  reward_scaling: 1.0

  max_grad_norm: 0.5
  clipping_epsilon: 0.2
  normalize_observations: true
  action_repeat: 1
  gae_lambda: 0.95

network:
  policy_hidden_layers: [256, 256, 128]
  value_hidden_layers: [256, 256, 128]

logging:
  use_wandb: true
  wandb_project: "wildrobot-amp"
  wandb_entity: null
  wandb_tags: ["phase1_rebase", "step2_alternation_metrics"]

rendering:
  render_videos: true
  render_height: 480
  render_width: 640

checkpointing:
  save_interval: 500000
  keep_checkpoints: 5
  load_checkpoint: "saved_checkpoints/phase0_final_policy.pkl"

validation:
  metrics:
    reward: "eval/episode_reward"
    episode_length: "eval/avg_episode_length"
  thresholds:
    reward_tolerance: 0.10
    length_tolerance: 0.10
    determinism_threshold: 0.01
