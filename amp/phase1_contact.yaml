# WildRobot AMP - Phase 1: Contact Rewards
# Enhanced reward shaping with foot contact and sliding penalties
# Physics-based approach (no mocap data required)

env:
  terrain: "flat"
  ctrl_dt: 0.02
  sim_dt: 0.002

  # Velocity command
  velocity_command_mode: "range"
  target_velocity: 0.7
  min_velocity: 0.5
  max_velocity: 1.0

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Phase signal (critical for contact rewards)
  use_phase_signal: true
  phase_period: 40
  num_phase_clocks: 2

  # Termination conditions
  min_height: 0.2
  max_height: 0.7

# Metrics to track (explicit list - cleaner than filtering)
# These are the reward components we want to log (no _std variants)
# Note: "total" is excluded because we compute it ourselves from the components
tracked_reward_components:
  - tracking_exp_xy
  - tracking_lin_xy
  - tracking_exp_yaw
  - tracking_lin_yaw
  - z_velocity
  - roll_pitch_velocity
  - roll_pitch_position
  - joint_velocity
  - joint_acceleration
  - action_rate
  - nominal_joint_position
  - joint_position_limit
  - joint_torque
  - mechanical_power
  - foot_contact
  - foot_sliding
  - foot_air_time
  - existential

# Reward weights - Phase 1B (improved for stability)
# Updated 2025-11-28: Reduced velocity pressure to prevent collapse
# Changes from Phase 1A:
# - tracking_exp_xy: 25.0 → 20.0 (less speed pressure)
# - foot_contact: 5.0 → 10.0 (better gait timing)
# - foot_sliding: 10.0 → 25.0 (cleaner foot placement)
# - foot_air_time: 3.0 → 5.0 (better flight phase)
reward_weights:
  # Tracking sigma
  tracking_sigma: 0.25

  # === Velocity tracking (REDUCED to prevent collapse) ===
  tracking_exp_xy: 20.0      # Was 25.0 - reduced to prioritize gait quality
  tracking_lin_xy: 8.0       # Keep same
  tracking_exp_yaw: 5.0      # Keep same
  tracking_lin_yaw: 1.5      # Keep same

  # === Stability penalties (keep same) ===
  z_velocity: 0.1            # Reduced from 1.0 - was dominating reward
  roll_pitch_velocity: 0.05  # Was 0.01 - allow some rotation
  roll_pitch_position: 2.0   # Increased from 0.1 - prevent leaning back

  # === Smoothness penalties (keep same) ===
  nominal_joint_position: 0.0
  joint_position_limit: 2.5
  joint_velocity: 1e-6
  joint_acceleration: 0.0      # DISABLED: Bug found - was using qvel instead of acceleration
  action_rate: 0.001

  # === Energy penalties (keep same) ===
  joint_torque: 1e-7
  mechanical_power: 1e-5

  # === Phase 1B: ENHANCED contact rewards for better gait ===
  foot_contact: 10.0         # Was 5.0 - DOUBLED to encourage longer stance
  foot_sliding: 25.0         # Was 10.0 - INCREASED 2.5x to reduce sliding
  foot_air_time: 5.0         # Was 3.0 - INCREASED for better flight phase

  # === Existential penalty ===
  existential: 1.0

training:
  num_timesteps: 20000000
  num_evals: 200
  episode_length: 600
  seed: 0

# Quick verify mode - fast sanity check before full training
# Usage: Add --quick_verify flag to training script
quick_verify:
  enabled: false              # Set to true or use --quick_verify flag
  num_timesteps: 10000        # ~20 seconds on GPU (512 envs, 10K steps)
  num_evals: 2                # Just 2 eval runs
  episode_length: 100         # Shorter episodes
  num_envs: 128               # Fewer parallel envs (faster startup)
  num_eval_envs: 16
  use_wandb: false            # Skip logging for quick tests
  render_videos: false        # Skip video rendering
  save_checkpoints: false     # Skip checkpointing

ppo:
  num_envs: 512
  num_eval_envs: 64
  batch_size: 256
  unroll_length: 32
  num_minibatches: 16
  num_updates_per_batch: 4

  learning_rate: 0.0003
  entropy_cost: 0.01
  discounting: 0.99
  reward_scaling: 1.0

  max_grad_norm: 0.5
  clipping_epsilon: 0.2
  normalize_observations: true
  action_repeat: 1

network:
  policy_hidden_layers: [256, 256, 128]
  value_hidden_layers: [256, 256, 128]

logging:
  use_wandb: true
  wandb_project: "wildrobot-amp"
  wandb_entity: null
  wandb_tags: ["phase1", "contact_rewards"]

rendering:
  render_videos: true
  render_height: 480
  render_width: 640

checkpointing:
  save_interval: 1000000
  keep_checkpoints: 5

validation:
  metrics:
    reward: "eval/episode_reward"
    episode_length: "eval/avg_episode_length"
  thresholds:
    reward_tolerance: 0.10
    length_tolerance: 0.10
    determinism_threshold: 0.01
