#!/usr/bin/env python3
"""Analyze Tier 0+ Dataset - Compute Summary Metrics for Go/No-Go Decision.

v0.1.0: Step 2 of the AMP training plan.

This script analyzes the Tier 0+ dataset generated by playground_amp/data/gmr_to_physics_ref_data.py
and computes:
1. Total Tier 0+ seconds (KEY DECISION METRIC)
2. Distribution of harness reliance (mean and p95)
3. Contact alternation and slip rates
4. Feature distribution statistics

Go/No-Go Decision Criteria:
- >= 60 seconds: PROCEED with AMP training
- 30-60 seconds: BORDERLINE, can attempt but may overfit
- < 30 seconds: INSUFFICIENT, pivot to baseline RL walking

Usage:
    cd ~/projects/wildrobot

    # Analyze Tier 0+ dataset
    uv run python scripts/analyze_tier0plus_dataset.py \
        --dataset playground_amp/data/tier0plus/tier0plus_merged.pkl

    # Analyze with segment-level breakdown
    uv run python scripts/analyze_tier0plus_dataset.py \
        --dataset playground_amp/data/tier0plus/tier0plus_merged.pkl \
        --metrics playground_amp/data/tier0plus/segment_metrics.json \
        --verbose
"""

import argparse
import json
import pickle
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
from rich import print
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

console = Console()


# =============================================================================
# Dataset Analysis
# =============================================================================


def load_dataset(dataset_path: Path) -> Dict[str, Any]:
    """Load Tier 0+ merged dataset."""
    with open(dataset_path, "rb") as f:
        return pickle.load(f)


def load_segment_metrics(metrics_path: Path) -> Dict[str, Any]:
    """Load segment metrics JSON."""
    with open(metrics_path, "r") as f:
        return json.load(f)


def analyze_dataset(
    data: Dict[str, Any],
    verbose: bool = False,
) -> Dict[str, Any]:
    """Analyze Tier 0+ dataset and compute summary metrics.

    Args:
        data: Merged Tier 0+ dataset
        verbose: Print detailed analysis

    Returns:
        Dict with analysis results
    """
    features = data["features"]
    num_frames = data["num_frames"]
    duration_sec = data["duration_sec"]
    num_segments = data["num_segments"]
    num_clips = data["num_clips"]
    feature_dim = data["feature_dim"]

    # Feature indices (27-dim physics reference)
    n_joints = 8
    joint_pos_slice = slice(0, n_joints)
    joint_vel_slice = slice(n_joints, 2 * n_joints)
    root_lin_vel_slice = slice(16, 19)
    root_ang_vel_slice = slice(19, 22)
    root_height_idx = 22
    foot_contacts_slice = slice(23, 27)

    # Feature statistics
    feature_mean = features.mean(axis=0)
    feature_std = features.std(axis=0)
    feature_min = features.min(axis=0)
    feature_max = features.max(axis=0)

    # Joint positions analysis
    joint_pos = features[:, joint_pos_slice]
    joint_vel = features[:, joint_vel_slice]

    # Root velocities analysis
    root_lin_vel = features[:, root_lin_vel_slice]
    root_ang_vel = features[:, root_ang_vel_slice]
    root_height = features[:, root_height_idx]

    # Contact analysis
    contacts = features[:, foot_contacts_slice]
    contact_rates = contacts.mean(axis=0)

    # Walking pattern analysis
    left_contact = (contacts[:, 0] > 0.5) | (contacts[:, 1] > 0.5)  # L_toe or L_heel
    right_contact = (contacts[:, 2] > 0.5) | (contacts[:, 3] > 0.5)  # R_toe or R_heel

    both_contact = left_contact & right_contact
    no_contact = ~left_contact & ~right_contact
    alternating = (left_contact ^ right_contact)  # XOR - exactly one foot

    both_rate = both_contact.mean()
    no_rate = no_contact.mean()
    alternating_rate = alternating.mean()

    # Contact transitions (gait cycle indicator)
    contact_transitions = np.diff((left_contact.astype(int) + 2 * right_contact.astype(int)))
    num_transitions = (contact_transitions != 0).sum()
    transition_rate = num_transitions / max(1, num_frames - 1)

    # Velocity statistics
    forward_vel = root_lin_vel[:, 0]  # X velocity (forward in heading frame)
    lateral_vel = root_lin_vel[:, 1]  # Y velocity (lateral)
    vertical_vel = root_lin_vel[:, 2]  # Z velocity (vertical)

    analysis = {
        # Key decision metrics
        "total_duration_sec": duration_sec,
        "num_frames": num_frames,
        "num_segments": num_segments,
        "num_clips": num_clips,
        "feature_dim": feature_dim,
        # Contact patterns
        "contact_rates": {
            "L_toe": float(contact_rates[0]),
            "L_heel": float(contact_rates[1]),
            "R_toe": float(contact_rates[2]),
            "R_heel": float(contact_rates[3]),
        },
        "both_contact_rate": float(both_rate),
        "no_contact_rate": float(no_rate),
        "alternating_rate": float(alternating_rate),
        "transition_rate": float(transition_rate),
        # Root height
        "height_mean": float(root_height.mean()),
        "height_std": float(root_height.std()),
        "height_min": float(root_height.min()),
        "height_max": float(root_height.max()),
        # Velocities
        "forward_vel_mean": float(forward_vel.mean()),
        "forward_vel_std": float(forward_vel.std()),
        "lateral_vel_std": float(np.abs(lateral_vel).std()),
        "vertical_vel_std": float(vertical_vel.std()),
        # Feature statistics
        "feature_mean_range": (float(feature_mean.min()), float(feature_mean.max())),
        "feature_std_range": (float(feature_std.min()), float(feature_std.max())),
        # Joint statistics
        "joint_pos_range": (float(joint_pos.min()), float(joint_pos.max())),
        "joint_vel_range": (float(joint_vel.min()), float(joint_vel.max())),
        # Data quality
        "has_nan": bool(np.any(np.isnan(features))),
        "has_inf": bool(np.any(np.isinf(features))),
        "min_std_dim": int(np.argmin(feature_std)),
        "min_std_value": float(feature_std.min()),
    }

    return analysis


def analyze_segments(
    segment_data: Dict[str, Any],
    verbose: bool = False,
) -> Dict[str, Any]:
    """Analyze segment-level metrics.

    Args:
        segment_data: Segment metrics JSON data
        verbose: Print detailed analysis

    Returns:
        Dict with segment analysis
    """
    clips = segment_data["clips"]
    summary = segment_data["summary"]

    # Collect Tier 0+ segment metrics
    all_segments = []
    for clip in clips:
        for seg in clip["segments"]:
            if seg["is_tier0plus"]:
                all_segments.append(seg)

    if not all_segments:
        return {
            "num_tier0plus_segments": 0,
            "error": "No Tier 0+ segments found",
        }

    # Harness reliance distribution
    p95_harness = [s["p95_harness"] for s in all_segments]
    mean_harness = [s["mean_harness"] for s in all_segments]

    # Load support distribution
    mean_load = [s["mean_load_support"] for s in all_segments]
    min_load = [s["min_load_support"] for s in all_segments]

    # Orientation stability
    p95_pitch = [s["p95_pitch"] for s in all_segments]
    p95_roll = [s["p95_roll"] for s in all_segments]

    # Contact patterns
    alternating = [s["alternating_contact_rate"] for s in all_segments]
    both_contact = [s["both_contact_rate"] for s in all_segments]
    no_contact = [s["no_contact_rate"] for s in all_segments]

    # Segment durations
    durations = [s["duration_sec"] for s in all_segments]

    analysis = {
        "num_tier0plus_segments": len(all_segments),
        "total_tier0plus_sec": summary["tier0plus_seconds"],
        # Harness reliance
        "harness_p95_mean": float(np.mean(p95_harness)),
        "harness_p95_max": float(np.max(p95_harness)),
        "harness_p95_min": float(np.min(p95_harness)),
        "harness_mean_mean": float(np.mean(mean_harness)),
        # Load support
        "load_mean_mean": float(np.mean(mean_load)),
        "load_mean_min": float(np.min(mean_load)),
        "load_min_mean": float(np.mean(min_load)),
        # Orientation
        "pitch_p95_mean": float(np.mean(p95_pitch)),
        "pitch_p95_max": float(np.max(p95_pitch)),
        "roll_p95_mean": float(np.mean(p95_roll)),
        "roll_p95_max": float(np.max(p95_roll)),
        # Contact patterns
        "alternating_mean": float(np.mean(alternating)),
        "both_contact_mean": float(np.mean(both_contact)),
        "no_contact_mean": float(np.mean(no_contact)),
        # Segment durations
        "segment_duration_mean": float(np.mean(durations)),
        "segment_duration_min": float(np.min(durations)),
        "segment_duration_max": float(np.max(durations)),
    }

    return analysis


# =============================================================================
# Report Generation
# =============================================================================


def print_go_nogo_decision(total_seconds: float):
    """Print the go/no-go decision based on total Tier 0+ seconds."""
    print(f"\n{'='*70}")
    print("[bold]GO/NO-GO DECISION[/bold]")
    print(f"{'='*70}")

    print(f"\n  Total Tier 0+ duration: [bold cyan]{total_seconds:.1f} seconds[/bold cyan]")

    if total_seconds >= 60:
        decision = "[bold green]✓ PROCEED[/bold green]"
        recommendation = """
  Recommendation: Proceed with micro-train smoke test

  Next steps:
  1. Run micro-train smoke test with Tier 0+ dataset
  2. Check discriminator AUC, AMP reward variance, feature stats
  3. If smoke test passes, start longer training run
"""
    elif total_seconds >= 30:
        decision = "[bold yellow]⚠ BORDERLINE[/bold yellow]"
        recommendation = """
  Recommendation: Can attempt training, but monitor closely

  Risks:
  - May overfit discriminator to limited reference data
  - Training may be unstable

  Mitigations:
  1. Use lower discriminator learning rate
  2. Increase discriminator regularization
  3. Monitor for early saturation

  Alternative: Generate more Tier 0+ data or relax thresholds slightly
"""
    else:
        decision = "[bold red]✗ INSUFFICIENT[/bold red]"
        recommendation = """
  Recommendation: Pivot to baseline RL walking first

  Path B approach:
  1. Train with task rewards only (velocity, stability, energy)
  2. Once stable walking achieved, collect rollouts
  3. Add AMP with Tier 0+ references at low weight
  4. Gradually ramp up AMP weight

  Alternative: Try relaxing Tier 0+ thresholds to generate more data
"""

    print(f"\n  Decision: {decision}")
    print(recommendation)


def print_dataset_analysis(analysis: Dict[str, Any]):
    """Print dataset analysis results."""
    print(f"\n{'='*70}")
    print("[bold]TIER 0+ DATASET ANALYSIS[/bold]")
    print(f"{'='*70}")

    # Basic stats
    print(f"\n[bold]Dataset Overview:[/bold]")
    print(f"  Total frames: {analysis['num_frames']:,}")
    print(f"  Total duration: {analysis['total_duration_sec']:.2f} seconds")
    print(f"  Number of segments: {analysis['num_segments']}")
    print(f"  Source clips: {analysis['num_clips']}")
    print(f"  Feature dimension: {analysis['feature_dim']}")

    # Data quality
    print(f"\n[bold]Data Quality:[/bold]")
    if analysis["has_nan"]:
        print(f"  [red]✗ Contains NaN values![/red]")
    else:
        print(f"  [green]✓ No NaN values[/green]")

    if analysis["has_inf"]:
        print(f"  [red]✗ Contains Inf values![/red]")
    else:
        print(f"  [green]✓ No Inf values[/green]")

    if analysis["min_std_value"] < 0.001:
        print(f"  [yellow]⚠ Low variance in dim {analysis['min_std_dim']} (std={analysis['min_std_value']:.6f})[/yellow]")
    else:
        print(f"  [green]✓ All dimensions have sufficient variance[/green]")

    # Contact patterns
    print(f"\n[bold]Contact Patterns:[/bold]")
    table = Table()
    table.add_column("Pattern", style="cyan")
    table.add_column("Rate", justify="right")

    cr = analysis["contact_rates"]
    table.add_row("L_toe", f"{cr['L_toe']:.1%}")
    table.add_row("L_heel", f"{cr['L_heel']:.1%}")
    table.add_row("R_toe", f"{cr['R_toe']:.1%}")
    table.add_row("R_heel", f"{cr['R_heel']:.1%}")
    table.add_row("---", "---")
    table.add_row("Both feet", f"{analysis['both_contact_rate']:.1%}")
    table.add_row("Alternating", f"[bold]{analysis['alternating_rate']:.1%}[/bold]")
    table.add_row("No contact", f"{analysis['no_contact_rate']:.1%}")

    console.print(table)

    # Gait quality indicator
    if analysis["alternating_rate"] > 0.3:
        print(f"  [green]✓ Good alternating contact pattern (walking-like)[/green]")
    elif analysis["alternating_rate"] > 0.1:
        print(f"  [yellow]⚠ Low alternating contact (may need more dynamic clips)[/yellow]")
    else:
        print(f"  [red]✗ Very low alternating contact (check motion quality)[/red]")

    # Root motion
    print(f"\n[bold]Root Motion:[/bold]")
    print(f"  Height: mean={analysis['height_mean']:.3f}m, std={analysis['height_std']:.3f}m")
    print(f"  Height range: [{analysis['height_min']:.3f}, {analysis['height_max']:.3f}]m")
    print(f"  Forward velocity: mean={analysis['forward_vel_mean']:.2f} m/s, std={analysis['forward_vel_std']:.2f}")

    # Joint statistics
    print(f"\n[bold]Joint Statistics:[/bold]")
    print(f"  Position range: [{analysis['joint_pos_range'][0]:.2f}, {analysis['joint_pos_range'][1]:.2f}] rad")
    print(f"  Velocity range: [{analysis['joint_vel_range'][0]:.2f}, {analysis['joint_vel_range'][1]:.2f}] rad/s")


def print_segment_analysis(seg_analysis: Dict[str, Any]):
    """Print segment-level analysis."""
    print(f"\n{'='*70}")
    print("[bold]SEGMENT-LEVEL ANALYSIS[/bold]")
    print(f"{'='*70}")

    if "error" in seg_analysis:
        print(f"\n  [red]{seg_analysis['error']}[/red]")
        return

    print(f"\n[bold]Segment Statistics:[/bold]")
    print(f"  Number of Tier 0+ segments: {seg_analysis['num_tier0plus_segments']}")
    print(f"  Total Tier 0+ duration: {seg_analysis['total_tier0plus_sec']:.2f}s")
    print(f"  Mean segment duration: {seg_analysis['segment_duration_mean']:.2f}s")
    print(f"  Segment duration range: [{seg_analysis['segment_duration_min']:.2f}, {seg_analysis['segment_duration_max']:.2f}]s")

    # Harness reliance
    print(f"\n[bold]Harness Reliance (across Tier 0+ segments):[/bold]")
    print(f"  Mean of p95(|F_stab|)/mg: [cyan]{seg_analysis['harness_p95_mean']:.1%}[/cyan]")
    print(f"  Max of p95(|F_stab|)/mg: {seg_analysis['harness_p95_max']:.1%}")
    print(f"  Mean harness reliance: {seg_analysis['harness_mean_mean']:.1%}")

    if seg_analysis["harness_p95_max"] <= 0.10:
        print(f"  [green]✓ Harness reliance within Tier 0+ bounds (≤10%)[/green]")
    elif seg_analysis["harness_p95_max"] <= 0.15:
        print(f"  [yellow]⚠ Some segments approaching harness limit[/yellow]")
    else:
        print(f"  [red]✗ Harness reliance exceeds bounds in some segments[/red]")

    # Load support
    print(f"\n[bold]Load Support (across Tier 0+ segments):[/bold]")
    print(f"  Mean of mean(ΣFn)/mg: [cyan]{seg_analysis['load_mean_mean']:.1%}[/cyan]")
    print(f"  Min of mean(ΣFn)/mg: {seg_analysis['load_mean_min']:.1%}")

    if seg_analysis["load_mean_min"] >= 0.90:
        print(f"  [green]✓ All segments meet 90% load support requirement[/green]")
    else:
        print(f"  [yellow]⚠ Some segments below 90% load support threshold[/yellow]")

    # Orientation stability
    print(f"\n[bold]Orientation Stability (across Tier 0+ segments):[/bold]")
    print(f"  Mean of p95(|pitch|): {seg_analysis['pitch_p95_mean']:.1f}°")
    print(f"  Max of p95(|pitch|): {seg_analysis['pitch_p95_max']:.1f}°")
    print(f"  Mean of p95(|roll|): {seg_analysis['roll_p95_mean']:.1f}°")
    print(f"  Max of p95(|roll|): {seg_analysis['roll_p95_max']:.1f}°")

    if seg_analysis["pitch_p95_max"] <= 20 and seg_analysis["roll_p95_max"] <= 20:
        print(f"  [green]✓ Orientation stable across all segments[/green]")
    else:
        print(f"  [yellow]⚠ Some segments have high tilt[/yellow]")

    # Contact patterns
    print(f"\n[bold]Contact Patterns (across Tier 0+ segments):[/bold]")
    print(f"  Mean alternating contact rate: {seg_analysis['alternating_mean']:.1%}")
    print(f"  Mean both-feet contact rate: {seg_analysis['both_contact_mean']:.1%}")
    print(f"  Mean no-contact rate: {seg_analysis['no_contact_mean']:.1%}")


# =============================================================================
# Main
# =============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Analyze Tier 0+ dataset for go/no-go decision"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="playground_amp/data/physics_based_ref/tier0plus_merged.pkl",
        help="Path to merged Tier 0+ dataset",
    )
    parser.add_argument(
        "--metrics",
        type=str,
        default=None,
        help="Path to segment_metrics.json (optional, for detailed analysis)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print detailed analysis",
    )
    parser.add_argument(
        "--json-output",
        type=str,
        default=None,
        help="Save analysis results to JSON file",
    )

    args = parser.parse_args()

    # Load dataset
    dataset_path = Path(args.dataset)
    if not dataset_path.exists():
        print(f"[red]Error: Dataset not found at {dataset_path}[/red]")
        print(f"\nRun gmr_to_physics_ref_data.py first:")
        print(f"  uv run python playground_amp/data/gmr_to_physics_ref_data.py --verbose")
        sys.exit(1)

    print(f"[bold blue]Loading Tier 0+ dataset:[/bold blue] {dataset_path}")
    data = load_dataset(dataset_path)

    # Analyze dataset
    analysis = analyze_dataset(data, verbose=args.verbose)

    # Print dataset analysis
    print_dataset_analysis(analysis)

    # Load and analyze segment metrics if provided
    seg_analysis = None
    if args.metrics:
        metrics_path = Path(args.metrics)
    else:
        # Try to find segment_metrics.json in same directory
        metrics_path = dataset_path.parent / "segment_metrics.json"

    if metrics_path.exists():
        print(f"\n[bold blue]Loading segment metrics:[/bold blue] {metrics_path}")
        segment_data = load_segment_metrics(metrics_path)
        seg_analysis = analyze_segments(segment_data, verbose=args.verbose)
        print_segment_analysis(seg_analysis)

    # Print go/no-go decision
    print_go_nogo_decision(analysis["total_duration_sec"])

    # Save to JSON if requested
    if args.json_output:
        output = {
            "dataset_analysis": analysis,
            "segment_analysis": seg_analysis,
            "decision": {
                "total_tier0plus_seconds": analysis["total_duration_sec"],
                "status": (
                    "proceed" if analysis["total_duration_sec"] >= 60
                    else "borderline" if analysis["total_duration_sec"] >= 30
                    else "insufficient"
                ),
            },
        }
        with open(args.json_output, "w") as f:
            json.dump(output, f, indent=2)
        print(f"\n[dim]Analysis saved to: {args.json_output}[/dim]")

    # Exit code
    if analysis["total_duration_sec"] >= 30:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()
