# WildRobot Locomotion Training Configuration
# Default balanced configuration for training
# Optimized for 12GB GPU (NVIDIA RTX 4070/5070)

env:
  terrain: "flat"              # Terrain type: "flat" or "rough"
  ctrl_dt: 0.02               # Control frequency: 50Hz
  sim_dt: 0.002               # Simulation frequency: 500Hz

  # Termination conditions (matches loco-mujoco's HeightBasedTerminalStateHandler)
  terminal_state_params:
    min_height: 0.2           # Minimum height before termination (fallen)
    max_height: 0.7           # Maximum height before termination (jumping/unstable)

  # Reward parameters (similar to loco_mujoco's LocomotionReward)
  reward_params:
    # Velocity tracking (primary goal - exponential + linear dual rewards)
    tracking_w_exp_xy: 15.0       # Exponential reward for XY velocity tracking
    tracking_w_lin_xy: 5.0        # Linear reward for XY velocity tracking
    tracking_w_exp_yaw: 4.0       # Exponential reward for yaw velocity tracking
    tracking_w_lin_yaw: 1.0       # Linear reward for yaw velocity tracking
    tracking_sigma: 0.25          # Tolerance for exponential rewards

    # Stability penalties (secondary - keep robot upright and stable)
    z_vel_coeff: 2.0              # Penalize vertical velocity (bobbing)
    roll_pitch_vel_coeff: 0.08    # Penalize roll/pitch angular velocity (tipping)
    roll_pitch_pos_coeff: 0.3     # Penalize deviation from upright posture

    # Smooth motion penalties (tertiary - encourage natural movement)
    nominal_joint_pos_coeff: 0.005      # Keep joints near default pose
    joint_position_limit_coeff: 5.0     # Avoid joint limits
    joint_vel_coeff: 5e-5               # Penalize large joint velocities
    joint_acc_coeff: 2e-5               # Penalize joint accelerations
    action_rate_coeff: 0.02             # Penalize jerky actions

    # Energy efficiency penalties (minimal weight)
    joint_torque_coeff: 2e-7      # Penalize torque
    energy_coeff: 2e-5            # Penalize mechanical power (action * qvel)

training:
  num_timesteps: 20000000     # Total training steps (20M like loco_mujoco)
  num_evals: 200              # Number of evaluations during training
  episode_length: 600         # Max steps per episode (matches loco_mujoco horizon)
  seed: 0                     # Random seed

ppo:
  num_envs: 2048              # Number of parallel environments (same as loco_mujoco)
  num_eval_envs: 128          # Number of evaluation environments
  batch_size: 512             # Batch size for PPO updates (2048 envs * 64 steps / 256 batch)
  unroll_length: 64           # Number of steps to unroll (same as loco_mujoco num_steps)
  num_minibatches: 32         # Number of minibatches per update (matches loco_mujoco)
  num_updates_per_batch: 4    # Number of gradient updates per batch (update_epochs)

  learning_rate: 0.0003       # Adam learning rate (same as loco_mujoco)
  entropy_cost: 0.01          # Entropy bonus coefficient (same as loco_mujoco ent_coef)
  discounting: 0.99           # Discount factor (gamma - higher for locomotion)
  reward_scaling: 1.0         # Reward scaling factor (no scaling, reward is already normalized)

  max_grad_norm: 0.5          # Gradient clipping threshold (same as loco_mujoco)
  clipping_epsilon: 0.2       # PPO clipping epsilon (same as loco_mujoco clip_eps)
  normalize_observations: true # Normalize observations
  action_repeat: 1            # Action repeat (1 = no repeat)

network:
  policy_hidden_layers: [256, 256, 128]  # Policy network layers
  value_hidden_layers: [256, 256, 128]   # Value network layers

logging:
  use_wandb: true             # Use Weights & Biases
  wandb_project: "wildrobot"  # W&B project name
  wandb_entity: null          # W&B entity/team (null = personal)

rendering:
  render_videos: true         # Render evaluation video (only at end, loco-mujoco style)
  render_height: 480          # Video height
  render_width: 640           # Video width

checkpointing:
  save_interval: 1000000      # Save checkpoint every N steps
  keep_checkpoints: 5         # Number of checkpoints to keep

validation:
  # Metrics to track for validation (saved in checkpoint for automatic validation)
  # Format: {display_name: metric_key_in_training}
  metrics:
    reward: "eval/episode_reward"           # Episode reward during evaluation
    episode_length: "eval/avg_episode_length"  # Average episode length
    # Add more metrics here as needed, e.g.:
    # velocity: "eval/avg_velocity"
    # height: "eval/avg_height"
    # action_rate: "eval/avg_action_rate"

  # Validation thresholds (for pass/fail checks)
  thresholds:
    reward_tolerance: 0.10       # ±10% tolerance for reward consistency
    length_tolerance: 0.10       # ±10% tolerance for episode length
    determinism_threshold: 0.01  # Max difference for determinism check
