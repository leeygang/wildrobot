# WildRobot Locomotion Training Configuration
# Default balanced configuration for training

env:
  terrain: "flat"              # Terrain type: "flat" or "rough"
  ctrl_dt: 0.02               # Control frequency: 50Hz
  sim_dt: 0.002               # Simulation frequency: 500Hz

training:
  num_timesteps: 30000000     # Total training steps (30M)
  num_evals: 10               # Number of evaluations during training
  episode_length: 1000        # Max steps per episode
  seed: 0                     # Random seed

ppo:
  num_envs: 2048              # Number of parallel environments
  num_eval_envs: 128          # Number of evaluation environments
  batch_size: 512             # Batch size for PPO updates
  unroll_length: 10           # Number of steps to unroll
  num_minibatches: 8          # Number of minibatches per update
  num_updates_per_batch: 4    # Number of gradient updates per batch

  learning_rate: 0.0003       # Adam learning rate
  entropy_cost: 0.01          # Entropy bonus coefficient
  discounting: 0.97           # Discount factor (gamma)
  reward_scaling: 0.1         # Reward scaling factor

  max_grad_norm: 1.0          # Gradient clipping threshold
  clipping_epsilon: 0.2       # PPO clipping epsilon
  normalize_observations: true # Normalize observations
  action_repeat: 1            # Action repeat (1 = no repeat)

network:
  policy_hidden_layers: [256, 256, 128]  # Policy network layers
  value_hidden_layers: [256, 256, 128]   # Value network layers

logging:
  use_wandb: true             # Use Weights & Biases
  wandb_project: "wildrobot"  # W&B project name
  wandb_entity: null          # W&B entity/team (null = personal)

rendering:
  render_videos: true         # Render evaluation videos
  num_videos: 3               # Number of videos to render
  render_height: 480          # Video height
  render_width: 640           # Video width

checkpointing:
  save_interval: 1000000      # Save checkpoint every N steps
  keep_checkpoints: 5         # Number of checkpoints to keep
