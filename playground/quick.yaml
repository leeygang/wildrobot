# Quick Training Configuration
# For fast prototyping and testing (smoke test - should complete in ~2 minutes)

env:
  terrain: "flat"
  ctrl_dt: 0.02
  sim_dt: 0.002

  # === Action Filtering (Low-Pass Filter) ===
  # Prevents high-frequency stuttering by smoothing actions over time
  use_action_filter: true      # Enable action filtering
  action_filter_alpha: 0.7     # Filter coefficient (0.7 = 70% old action, 30% new action)

  # === Phase Signal (Gait Clock) ===
  # Provides periodic timing to help robot learn rhythmic stepping pattern
  use_phase_signal: true       # Enable phase signal in observations
  phase_period: 40             # Steps per gait cycle (40 steps = 0.8s at 50Hz)
  num_phase_clocks: 2          # Number of phase clocks (1 = single, 2 = left+right legs)

  # Termination conditions (matches loco-mujoco's HeightBasedTerminalStateHandler)
  terminal_state_params:
    min_height: 0.2          # Minimum height before termination (fallen)
    max_height: 0.7          # Maximum height before termination (jumping/unstable)

  # Reward parameters (similar to loco_mujoco's LocomotionReward)
  reward_params:
    # Velocity tracking (primary goal - exponential + linear dual rewards)
    # INCREASED to strongly encourage walking over standing
    tracking_w_exp_xy: 50.0       # Exponential reward for XY velocity tracking (was 15.0)
    tracking_w_lin_xy: 15.0        # Linear reward for XY velocity tracking (was 5.0)
    tracking_w_exp_yaw: 10.0       # Exponential reward for yaw velocity tracking (was 4.0)
    tracking_w_lin_yaw: 3.0        # Linear reward for yaw velocity tracking (was 1.0)
    tracking_sigma: 0.25          # Tolerance for exponential rewards

    # Stability penalties (secondary - keep robot upright and stable)
    # FURTHER DECREASED to allow natural walking dynamics (bobbing, rotation)
    z_vel_coeff: 0.1              # Penalize vertical velocity (allow bobbing for walking!)
    roll_pitch_vel_coeff: 0.01    # Penalize roll/pitch angular velocity (allow rotation)
    roll_pitch_pos_coeff: 0.05    # Penalize deviation from upright posture (keep loose)

    # Smooth motion penalties (tertiary - encourage natural movement)
    # MINIMIZED to allow dynamic stepping motions
    nominal_joint_pos_coeff: 0.0        # Don't penalize deviation from default pose
    joint_position_limit_coeff: 2.5     # Avoid joint limits (keep this)
    joint_vel_coeff: 1e-6               # Minimal penalty on joint velocity
    joint_acc_coeff: 1e-6               # Minimal penalty on joint acceleration
    action_rate_coeff: 0.001            # Minimal penalty on action rate

    # Energy efficiency penalties (minimal weight)
    joint_torque_coeff: 1e-7      # Penalize torque (was 2e-7)
    energy_coeff: 1e-5            # Penalize mechanical power (action * qvel) (was 2e-5)

training:
  num_timesteps: 100000       # 100K steps (VERY fast smoke test)
  num_evals: 2                # Just 2 evals to verify it works
  episode_length: 200         # Shorter episodes
  seed: 0

ppo:
  num_envs: 64                # Very few envs for 12GB GPU (64 * 5 = 320 samples)
  num_eval_envs: 16           # Fewer eval envs
  batch_size: 64              # Smaller batch (320 / 2 = 160, 160 / 64 = 2.5, use 64)
  unroll_length: 5            # Shorter unrolls
  num_minibatches: 2          # Fewer minibatches (320 samples / 2 = 160 per minibatch)
  num_updates_per_batch: 2    # Fewer updates

  learning_rate: 0.0003
  entropy_cost: 0.01
  discounting: 0.97
  reward_scaling: 0.1

  max_grad_norm: 1.0
  clipping_epsilon: 0.2
  normalize_observations: true
  action_repeat: 1

network:
  policy_hidden_layers: [128, 128]       # Smaller network
  value_hidden_layers: [128, 128]

logging:
  use_wandb: false            # Disable W&B for quick testing
  wandb_project: "wildrobot"
  wandb_entity: null

rendering:
  render_videos: false        # Disable videos for quick testing (very slow!)
  render_height: 480
  render_width: 640

checkpointing:
  save_interval: 1000000
  keep_checkpoints: 3

validation:
  # Metrics to track for validation (saved in checkpoint for automatic validation)
  # Format: {display_name: metric_key_in_training}
  metrics:
    reward: "eval/episode_reward"           # Episode reward during evaluation
    episode_length: "eval/avg_episode_length"  # Average episode length
    # Add more metrics here as needed, e.g.:
    # velocity: "eval/avg_velocity"
    # height: "eval/avg_height"
    # action_rate: "eval/avg_action_rate"

  # Validation thresholds (for pass/fail checks)
  thresholds:
    reward_tolerance: 0.10       # ±10% tolerance for reward consistency
    length_tolerance: 0.10       # ±10% tolerance for episode length
    determinism_threshold: 0.01  # Max difference for determinism check
