# Quick Training Configuration
# For fast prototyping and testing (smoke test - should complete in ~2 minutes)

env:
  terrain: "flat"
  ctrl_dt: 0.02
  sim_dt: 0.002

  # Termination conditions (matches loco-mujoco's HeightBasedTerminalStateHandler)
  terminal_state_params:
    min_height: 0.2          # Minimum height before termination (fallen)
    max_height: 0.7          # Maximum height before termination (jumping/unstable)

training:
  num_timesteps: 100000       # 100K steps (VERY fast smoke test)
  num_evals: 2                # Just 2 evals to verify it works
  episode_length: 200         # Shorter episodes
  seed: 0

ppo:
  num_envs: 256               # Fewer envs for faster compilation (must divide batch_size * num_minibatches)
  num_eval_envs: 32           # Fewer eval envs
  batch_size: 128             # Smaller batch (128 * 2 = 256, divisible by 256 envs)
  unroll_length: 5            # Shorter unrolls
  num_minibatches: 2          # Fewer minibatches
  num_updates_per_batch: 2    # Fewer updates

  learning_rate: 0.0003
  entropy_cost: 0.01
  discounting: 0.97
  reward_scaling: 0.1

  max_grad_norm: 1.0
  clipping_epsilon: 0.2
  normalize_observations: true
  action_repeat: 1

network:
  policy_hidden_layers: [128, 128]       # Smaller network
  value_hidden_layers: [128, 128]

logging:
  use_wandb: false            # Disable W&B for quick testing
  wandb_project: "wildrobot"
  wandb_entity: null

rendering:
  render_videos: false        # Disable videos for quick testing (very slow!)
  render_height: 480
  render_width: 640

checkpointing:
  save_interval: 1000000
  keep_checkpoints: 3

validation:
  # Metrics to track for validation (saved in checkpoint for automatic validation)
  # Format: {display_name: metric_key_in_training}
  metrics:
    reward: "eval/episode_reward"           # Episode reward during evaluation
    episode_length: "eval/avg_episode_length"  # Average episode length
    # Add more metrics here as needed, e.g.:
    # velocity: "eval/avg_velocity"
    # height: "eval/avg_height"
    # action_rate: "eval/avg_action_rate"

  # Validation thresholds (for pass/fail checks)
  thresholds:
    reward_tolerance: 0.10       # ±10% tolerance for reward consistency
    length_tolerance: 0.10       # ±10% tolerance for episode length
    determinism_threshold: 0.01  # Max difference for determinism check
