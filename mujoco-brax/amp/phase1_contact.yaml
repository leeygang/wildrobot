# WildRobot AMP - Phase 1: Contact Rewards (v3 - Phase 1 Quick Fix - 2025-12-06)
# PHASE 1 TEST: Reduce velocity commands to match robot capability (0.3-0.6 m/s)
# Previous issue: Commands 0.7-1.0 m/s exceeded robot max capability (~0.5 m/s)
# This caused degradation even with 100x velocity bonus
# Testing if realistic commands prevent degradation

env:
  terrain: "flat"
  ctrl_dt: 0.02
  sim_dt: 0.002

  # Velocity command - FIXED 2025-12-06: Match commands to robot capability
  # ISSUE: Commands at 0.7-1.0 m/s were unrealistic (robot max ~0.5 m/s)
  # FIX: Reduce to achievable range 0.3-0.6 m/s
  velocity_command_mode: "range"
  target_velocity: 0.5
  min_velocity: 0.3        # CHANGED: 0.7 → 0.3 (achievable target)
  max_velocity: 0.6        # CHANGED: 1.0 → 0.6 (matches capability)

  # Action filtering
  use_action_filter: true
  action_filter_alpha: 0.7

  # Phase signal (critical for contact rewards)
  use_phase_signal: true
  phase_period: 40
  num_phase_clocks: 2

  # Termination conditions
  min_height: 0.2
  max_height: 0.7

# Metrics to track (explicit list - cleaner than filtering)
tracked_reward_components:
  - tracking_exp_xy
  - tracking_lin_xy
  - tracking_exp_yaw
  - tracking_lin_yaw
  - forward_velocity_bonus
  - velocity_threshold_penalty
  - z_velocity
  - roll_pitch_velocity
  - roll_pitch_position
  - joint_velocity
  - joint_acceleration
  - action_rate
  - nominal_joint_position
  - joint_position_limit
  - joint_torque
  - mechanical_power
  - foot_contact
  - foot_sliding
  - foot_air_time
  - existential

# Reward weights - Phase 1C (Hybrid: Balanced rewards & penalties)
# Updated 2025-11-30: Reduce foot_sliding penalty to allow learning
# Changes from previous:
# - tracking_exp_xy: 35.0 → 100.0 (3x increase - task reward dominates!)
# - tracking_lin_xy: 14.0 → 50.0 (3.5x increase)
# - joint_acceleration: 1e-4 → 1e-8 (100x reduction - was causing 80k penalty)
# - foot_sliding: 25.0 → 12.0 (2025-11-30: Robot is "ice skating" at 5m/s sliding
#   vs 0.5m/s forward. This is normal in early training. Reduce weight to allow
#   exploration while still penalizing. Robot will learn to reduce sliding naturally.)
# Goal: Tracking reward >> penalties, creating clear "go fast" signal
# Expected: Net reward closer to 0 or positive
reward_weights:
  # Tracking sigma
  tracking_sigma: 0.25

  # === OPTION 3 (HYBRID FIX) - UPDATED 2025-12-01 Phase 1G-v2 ===
  # CRITICAL: Fix exp(-error²) being too forgiving (61% reward for 100% error!)
  #
  # Phase 1G FAILED: Original params too harsh (penalty -17.61, reward -9.09)
  # Robot trapped in punishment zone, couldn't escape to higher velocities
  #
  # FIX #1: Stricter tracking formula with steepness factor
  #   OLD: exp(-error²) gives 61% reward for 100% error
  #   NEW: exp(-3 × error²) gives 14% reward for 100% error
  tracking_steepness: 3.0  # Multiply error² by this before exp()

  # FIX #2: Velocity threshold penalty - REDUCED (2025-12-06)
  #   PROBLEM: 10x weight caused policy to move slow to avoid penalty
  #            Even though tracking suffered, avoiding 10x penalty was worth it
  #   SOLUTION: Reduce to 1.0x - let tracking rewards handle velocity naturally
  #            Tracking exp/lin already penalize velocity errors strongly
  velocity_threshold: 0.12
  velocity_threshold_penalty: 1.0    # CHANGED: 10.0 → 1.0 (10x reduction)
  # Decay schedule: Improved for better exploration and smoother curriculum
  velocity_threshold_penalty_decay_start: 500000   # Delayed start (was 150K) - allows skill formation
  velocity_threshold_penalty_decay_steps: 1000000  # Extended duration (was 300K) - smoother transitions
  velocity_threshold_penalty_min_scale: 0.3

  # Combined Effect (Option 3 v2 - GENTLER):
  #   Standing (0.0 m/s):  reward +4.4 (was -12.9, now positive but small)
  #   Slow (0.034 m/s):    reward +9.9 (was -9.1, NOW POSITIVE!)
  #   Medium (0.15 m/s):   reward +18.5 (escape achieved!)
  #   Walking (0.7 m/s):   reward +101.4 (GREAT!)
  # NEW GRADIENT: 23x (0.0→0.7), still strong but robot can explore!

  # === Velocity tracking - REDUCED to baseline levels ===
  # Issue 2025-11-30: Robot falling immediately with high tracking rewards
  # Reduced from 100/50 back to baseline 50/15 to allow learning balance first
  tracking_exp_xy: 25.0      # FURTHER REDUCED: Limit passive reward when far from target
  tracking_lin_xy: 12.0      # Slight reduction to maintain gradient without dominance
  tracking_exp_yaw: 5.0      # Keep same
  tracking_lin_yaw: 1.5      # Keep same

  # === Forward Velocity Bonus - DOUBLED (2025-12-06) ===
  # PROBLEM: At 50x, slow (0.3 m/s) vs fast (0.7 m/s) gap only +20 points
  #          Policy chose slow because difficulty penalty > reward advantage
  # SOLUTION: Double to 100x creates +40 point gap (exceeds all penalties)
  #   At 0.7 m/s: 100.0 × 0.7 = +70 reward (was +35)
  #   At 0.3 m/s: 100.0 × 0.3 = +30 reward (was +15)
  #   Gap: 40 points (was 20) - now worth the difficulty of fast walking
  forward_velocity_bonus: 100.0    # CHANGED: 50.0 → 100.0

  # === Stability penalties - REVERTED to baseline ===
  # Issue 2025-11-30: 40x stronger roll_pitch_position (2.0) was preventing
  # natural walking dynamics. Robot needs to lean/tilt for weight transfer!
  z_velocity: 0.1
  roll_pitch_velocity: 0.01      # REDUCED: Was 0.05 → 0.01 (back to baseline)
  roll_pitch_position: 0.05      # REDUCED: Was 2.0 → 0.05 (back to baseline, 40x reduction!)

  # === Smoothness penalties - Keep minimal to allow learning ===
  nominal_joint_position: 0.0
  joint_position_limit: 2.5
  joint_velocity: 0.0          # DISABLED: Don't double-penalize with acceleration
  joint_acceleration: 1e-8     # MINIMAL: 105k × 1e-8 = 0.001 (negligible)
  action_rate: 0.001           # Keep baseline

  # === Energy penalties (REDUCED from Phase 1B) ===
  joint_torque: 7e-8         # Was 1e-7 - reduce energy penalty
  mechanical_power: 7e-6     # Was 1e-5 - reduce energy penalty

  # === Contact rewards - INCREASED FOOT SLIDING (2025-12-06) ===
  # foot_contact: Restored to 5.0 (matches phase0)
  # foot_sliding: INCREASED 2.0 → 5.0 to enforce clean gait
  #   PROBLEM: Sliding grew from 5.59 to 6.13 penalty as gait degraded
  #            At 2.0x weight, only 11.2-12.3 total penalty - easily ignored
  #   SOLUTION: Match foot_contact weight at 5.0x (critical for sim2real)
  #            Real robot feet will slip/fall if this isn't learned in sim
  foot_contact: 5.0
  foot_sliding: 5.0          # CHANGED: 2.0 → 5.0 (2.5x increase)
  foot_air_time: 0.0

  # === Existential penalty ===
  existential: 0.5
  
  # Tracking gate removed from Phase 1 configs; gating logic deprecated.

training:
  num_timesteps: 2000000    # PHASE 1 TEST: 2M steps for quick verification (~30-45 min)
  num_evals: 40             # Eval every 50K steps
  episode_length: 600
  seed: 0

# Quick verify mode - fast sanity check before full training
# Usage: Add --quick_verify flag to training script
quick_verify:
  enabled: false              # Set to true or use --quick_verify flag
  num_timesteps: 1500000      # 1.5M steps to test checkpoint saving & schedule
  num_evals: 6                # More evals to see checkpoint saves (every 250K steps)
  episode_length: 100         # Shorter episodes for faster testing
  num_envs: 128               # Fewer parallel envs (faster startup)
  num_eval_envs: 16
  use_wandb: false            # Skip logging for quick tests
  render_videos: false
  save_checkpoints: true      # Enable to test checkpoint saving

ppo:
  num_envs: 512
  num_eval_envs: 64
  batch_size: 256
  unroll_length: 32
  num_minibatches: 16
  num_updates_per_batch: 4

  learning_rate: 0.0003
  entropy_cost: 0.01
  discounting: 0.99
  reward_scaling: 1.0

  max_grad_norm: 0.5
  clipping_epsilon: 0.2
  normalize_observations: true
  action_repeat: 1

network:
  policy_hidden_layers: [256, 256, 128]
  value_hidden_layers: [256, 256, 128]

logging:
  use_wandb: true
  wandb_project: "wildrobot-amp"
  wandb_entity: null
  wandb_tags: ["phase1", "contact_rewards"]

rendering:
  render_videos: true
  render_height: 480
  render_width: 640

checkpointing:
  save_interval: 500000      # Save checkpoint every N steps (500K default)
  save_best: true            # Save best checkpoint based on reward
  
  # Warm-start from phase0 foundation training
  # Use: --load_checkpoint flag will load this by default if not overridden
  load_checkpoint: "saved_checkpoints/phase0_final_policy.pkl"

validation:
  metrics:
    reward: "eval/episode_reward"
    episode_length: "eval/avg_episode_length"
  thresholds:
    reward_tolerance: 0.10
    length_tolerance: 0.10
    determinism_threshold: 0.01
